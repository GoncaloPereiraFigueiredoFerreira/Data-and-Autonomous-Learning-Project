{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests trainning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score,classification_report,accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf = pd.read_csv(\"training_data_treated.csv\")\n",
    "testDf = pd.read_csv(\"test_data_treated.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def featureSelect(trainDf):\n",
    "    return trainDf[[\"delay_in_seconds\",\"avg_atm_pressure\",\"record_date_month\",\"record_date_day\",\"dayYear\",\"record_date_isWeekend\",\"record_date_hour\",\"N101\",\"Affected_Count\"]]\n",
    "    #return trainDf[[\"delay_in_seconds\",\"avg_rain\",\"avg_wind_speed\",\"avg_temperature\",\"luminosity\",\"avg_humidity\",\"avg_atm_pressure\",\"record_date_month\",\"record_date_day\",\"record_date_hour\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "x  = featureSelect(trainDf)\n",
    "y = trainDf[[\"incidents\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature select testing using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#pca = PCA(n_components=10)\n",
    "#principalComponents = pca.fit_transform(x)\n",
    "#x = pd.DataFrame(data = principalComponents\n",
    "#             , columns = ['principal component 1', 'principal component 2','principal component 3','principal component 4','principal component 5','principal component 6','principal component 7','principal component 8','principal component 9','principal component 10'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForestModel():\n",
    "    return RandomForestClassifier(random_state=13122001,\n",
    "                            n_estimators=1777,\n",
    "                            criterion='entropy',\n",
    "                            max_features='auto',\n",
    "                            class_weight='balanced',\n",
    "                            max_depth= 38,\n",
    "                            min_samples_split=3,\n",
    "                            min_samples_leaf=1,\n",
    "                            n_jobs=-1,\n",
    "                            bootstrap= False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for hyparameter tunning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 1000, stop = 2000, num = 20)]\n",
    "\n",
    "# Criterion of the quality of a split\n",
    "criterion = ['gini','entropy', 'log_loss']\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt','log2']\n",
    "\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(20, 150, num = 10)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [1, 2, 4, 6]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1,2,4]\n",
    "\n",
    "n_jobs = [-1]\n",
    "\n",
    "random_state = [13122001]\n",
    "\n",
    "class_weight = ['balanced']\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [False]# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'criterion' : criterion,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'n_jobs' :n_jobs,\n",
    "               'random_state' : random_state,\n",
    "               'class_weight' : class_weight,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "clf = randomForestModel()\n",
    "# Random search of parameters, using 3 fold cross validation,   \n",
    "# search across 100 different combinations, and use all available cores\n",
    "#rf_random = GridSearchCV(estimator = clf, param_grid = random_grid, cv = 3, verbose=2)# Fit the random search model\n",
    "\n",
    "#Activate this line to perform hyperparameter search\n",
    "#rf_random.fit(x, y.values.ravel())\n",
    "#print(rf_random.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small test using SMOTE oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "\n",
    "#oversample = SMOTE(sampling_strategy={0:1398,1:762,2:700,3:762,4:762},random_state=13122001)\n",
    "#x_over, y_over = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "#clf.fit(x_over,y_over.values.ravel())\n",
    "#predictions = clf.predict(X_test)\n",
    "\n",
    "# print classification report\n",
    "#print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small test using NearMiss undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply near miss\n",
    "#from imblearn.under_sampling import NearMiss\n",
    "#nr = NearMiss(sampling_strategy={0:1100,1:503,2:412,3:762,4:425})\n",
    "\n",
    "#X_train_miss, y_train_miss = nr.fit_resample(X_train, y_train)\n",
    "\n",
    "#clf.fit(X_train_miss,y_train_miss.values.ravel())\n",
    "#predictions = clf.predict(X_test)\n",
    "# print classification report\n",
    "#print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation and training with 10 KFolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Acc: 0.932 F1Score: 0.9318012707205786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       388\n",
      "           1       0.90      0.93      0.92       150\n",
      "           2       0.88      0.84      0.86       128\n",
      "           3       0.92      0.88      0.90       229\n",
      "           4       0.86      0.91      0.88       105\n",
      "\n",
      "    accuracy                           0.93      1000\n",
      "   macro avg       0.91      0.91      0.91      1000\n",
      "weighted avg       0.93      0.93      0.93      1000\n",
      "\n",
      "[2] Acc: 0.944 F1Score: 0.9441897049163991\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       414\n",
      "           1       0.94      0.97      0.95       150\n",
      "           2       0.84      0.86      0.85        96\n",
      "           3       0.91      0.90      0.90       213\n",
      "           4       0.91      0.92      0.92       127\n",
      "\n",
      "    accuracy                           0.94      1000\n",
      "   macro avg       0.92      0.93      0.92      1000\n",
      "weighted avg       0.94      0.94      0.94      1000\n",
      "\n",
      "[3] Acc: 0.936 F1Score: 0.9355911509228558\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       405\n",
      "           1       0.90      0.93      0.91       139\n",
      "           2       0.90      0.82      0.86       131\n",
      "           3       0.89      0.93      0.91       204\n",
      "           4       0.95      0.88      0.91       121\n",
      "\n",
      "    accuracy                           0.94      1000\n",
      "   macro avg       0.92      0.91      0.92      1000\n",
      "weighted avg       0.94      0.94      0.94      1000\n",
      "\n",
      "[4] Acc: 0.938 F1Score: 0.9380774824842358\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       412\n",
      "           1       0.91      0.89      0.90       142\n",
      "           2       0.85      0.85      0.85       105\n",
      "           3       0.90      0.93      0.91       220\n",
      "           4       0.94      0.93      0.93       121\n",
      "\n",
      "    accuracy                           0.94      1000\n",
      "   macro avg       0.92      0.92      0.92      1000\n",
      "weighted avg       0.94      0.94      0.94      1000\n",
      "\n",
      "[5] Acc: 0.95 F1Score: 0.9501106666080855\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       409\n",
      "           1       0.91      0.91      0.91       137\n",
      "           2       0.88      0.91      0.90       118\n",
      "           3       0.94      0.92      0.93       207\n",
      "           4       0.92      0.95      0.93       129\n",
      "\n",
      "    accuracy                           0.95      1000\n",
      "   macro avg       0.93      0.94      0.93      1000\n",
      "weighted avg       0.95      0.95      0.95      1000\n",
      "\n",
      "Average: 0.9400000000000001\n"
     ]
    }
   ],
   "source": [
    "scores =[]\n",
    "kf = KFold(n_splits=5,shuffle=True,random_state=13032001)  # we dont need random seed here if cross validation is desired\n",
    "counter =1\n",
    "\n",
    "for train,test in kf.split(x):\n",
    "    clf.fit(x.loc[train,:],y.loc[train,:].values.ravel())\n",
    "    score = clf.score(x.loc[test,:],y.loc[test,:])\n",
    "    scores.append(score)\n",
    "    y_predicted = clf.predict(x.loc[test,:])\n",
    "    f1 = f1_score(y.loc[test,:],y_predicted,average=\"weighted\")\n",
    "    print(\"[\"+ str(counter) +\"] Acc:\",score,\"F1Score:\", f1)\n",
    "    print(classification_report(y.loc[test,:],y_predicted))\n",
    "    counter+=1\n",
    "print(\"Average:\",np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = featureSelect(trainDf)\n",
    "y_train = trainDf[[\"incidents\"]]\n",
    "\n",
    "\n",
    "x_test =featureSelect(testDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a RF classifier\n",
    "clf = randomForestModel()\n",
    "\n",
    "# Training the model on the training dataset\n",
    "# fit function is used to train the model using the training sets as parameters\n",
    "clf.fit(x,y.values.ravel())\n",
    " \n",
    "# performing predictions on the test dataset\n",
    "y_pred = clf.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1206\n"
     ]
    }
   ],
   "source": [
    "f = open(\"Submissions/forestSub.csv\", \"w\")\n",
    "\n",
    "replace_map = {0:'None', 1:'Low', 2:'Medium',3:'High',4:'Very_High'}\n",
    "\n",
    "print(y_pred.size)\n",
    "\n",
    "\n",
    "f.write(\"RowId,Incidents\\n\")\n",
    "\n",
    "for i in range(y_pred.size):\n",
    "    f.write(str(i+1))\n",
    "    f.write(\",\")\n",
    "    f.write(replace_map[y_pred[i]])\n",
    "    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many lines changed compared to the best submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "count = 0\n",
    "filename1 = \"Submissions/bestSub.csv\"\n",
    "filename2 = \"Submissions/forestSub.csv\"\n",
    "count2=0\n",
    "\n",
    "with open(filename1) as file1, open(filename2) as file2:\n",
    "    for line_file_1, line_file_2 in zip(file1, file2):\n",
    "        if line_file_1 != line_file_2:\n",
    "            count += 1\n",
    "        else: count2+=1\n",
    "\n",
    "print(count)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bb50656a97d90f3364306f1185a9a91686dc76292645ad8896f0c9fab5293942"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
